#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ ï¿½ï¿½ØµÙ†ÙˆØ¹ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ùˆ ØªØ´Ø®ÛŒØµ Ù…Ø§ÛŒÙ†Ø±Ù‡Ø§
ØªÙˆØ³Ø·: Ø¹Ø±ÙØ§Ù† Ø±Ø¬Ø¨ÛŒ (Erfan Rajabi)

Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key:
1. Scikit-learn Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯Ùˆ
2. TensorFlow Lite Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªØµØ§ÙˆÛŒØ±
3. OpenCV Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±
4. NumPy/SciPy Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³ÛŒÚ¯Ù†Ø§Ù„
"""

import numpy as np
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import OneClassSVM
import joblib
import os

class FreeAIAnalyzer:
    """Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù†"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.training_data = {
            'network_patterns': [],
            'rf_signatures': [],
            'power_patterns': [],
            'thermal_patterns': []
        }
        
        # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡
        self.anomaly_detector = None
        self.pattern_classifier = None
        self.clustering_model = None
        
        # Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ
        self.detection_thresholds = {
            'anomaly_score': -0.1,
            'classification_confidence': 0.7,
            'cluster_density': 0.3
        }
        
        self._initialize_models()
        
    def _initialize_models(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML"""
        print("ğŸ¤– Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...")
        
        # Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ
        self.anomaly_detector = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        
        # Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        self.pattern_classifier = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=10
        )
        
        # Ù…Ø¯Ù„ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        self.clustering_model = DBSCAN(
            eps=0.5,
            min_samples=5
        )
        
        # Ù…Ø¯Ù„ Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯
        self.pca_model = PCA(n_components=5)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.scaler = StandardScaler()
        
        # Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯ï¿½ï¿½
        self._train_with_synthetic_data()
        
        print("âœ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯")
    
    def _train_with_synthetic_data(self):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡"""
        print("ğŸ“š Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ...")
        
        # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
        normal_data, miner_data = self._generate_training_data()
        
        # ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        X = np.vstack([normal_data, miner_data])
        y = np.array([0] * len(normal_data) + [1] * len(miner_data))
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        X_scaled = self.scaler.fit_transform(X)
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ (ÙÙ‚Ø· Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø§Ø¯ÛŒ)
        self.anomaly_detector.fit(normal_data)
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        self.pattern_classifier.fit(X_scaled, y)
        
        # Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯
        self.pca_model.fit(X_scaled)
        
        print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯")
    
    def _generate_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡"""
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: [rf_power, magnetic_field, temperature, power_consumption, 
        #            network_traffic, connection_count, peak_frequency]
        
        # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø§Ø¯ÛŒ (ØºÛŒØ± Ù…Ø§ÛŒÙ†Ø±)
        normal_samples = 1000
        normal_data = np.random.normal(0, 1, (normal_samples, 7))
        
        # ØªÙ†Ø¸ÛŒÙ… Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ
        normal_data[:, 0] = np.random.uniform(-80, -60, normal_samples)  # RF power (dBm)
        normal_data[:, 1] = np.random.uniform(40, 60, normal_samples)    # Magnetic field (nT)
        normal_data[:, 2] = np.random.uniform(20, 35, normal_samples)    # Temperature (Â°C)
        normal_data[:, 3] = np.random.uniform(100, 500, normal_samples)  # Power (W)
        normal_data[:, 4] = np.random.uniform(10, 100, normal_samples)   # Network traffic (MB/h)
        normal_data[:, 5] = np.random.uniform(1, 20, normal_samples)     # Connections
        normal_data[:, 6] = np.random.uniform(1, 100, normal_samples)    # Peak frequency (MHz)
        
        # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§ÛŒÙ†Ø±
        miner_samples = 300
        miner_data = np.random.normal(0, 1, (miner_samples, 7))
        
        # ØªÙ†Ø¸ÛŒÙ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®ØµÙ‡ Ù…Ø§ÛŒÙ†Ø±
        miner_data[:, 0] = np.random.uniform(-50, -30, miner_samples)    # RF power Ø¨Ø§Ù„Ø§ØªØ±
        miner_data[:, 1] = np.random.uniform(50, 200, miner_samples)     # Magnetic field Ø¨Ø§Ù„Ø§ØªØ±
        miner_data[:, 2] = np.random.uniform(60, 90, miner_samples)      # Temperature Ø¨Ø§Ù„Ø§
        miner_data[:, 3] = np.random.uniform(1000, 5000, miner_samples)  # Power Ù…ØµØ±Ù Ø¨Ø§Ù„Ø§
        miner_data[:, 4] = np.random.uniform(500, 2000, miner_samples)   # Network traffic Ø¨Ø§Ù„Ø§
        miner_data[:, 5] = np.random.uniform(50, 200, miner_samples)     # Connections Ø²ÛŒØ§Ø¯
        miner_data[:, 6] = np.random.choice([13.56, 27.12, 433.92], miner_samples)  # ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
        
        return normal_data, miner_data
    
    def analyze_network_pattern(self, network_data: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒ Ø´Ø¨Ú©Ù‡ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
        print("ğŸ•¸ï¸ ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒ Ø´Ø¨Ú©Ù‡ Ø¨Ø§ AI...")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡
        features = self._extract_network_features(network_data)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        features_scaled = self.scaler.transform([features])
        
        # ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ
        anomaly_score = self.anomaly_detector.decision_function(features_scaled)[0]
        is_anomaly = self.anomaly_detector.predict(features_scaled)[0] == -1
        
        # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        miner_probability = self.pattern_classifier.predict_proba(features_scaled)[0][1]
        is_miner = miner_probability > self.detection_thresholds['classification_confidence']
        
        # Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ¬Ø³Ù…
        pca_features = self.pca_model.transform(features_scaled)[0]
        
        result = {
            'analysis_type': 'network_pattern',
            'timestamp': datetime.now().isoformat(),
            'features': {
                'rf_power': features[0],
                'magnetic_field': features[1],
                'temperature': features[2],
                'power_consumption': features[3],
                'network_traffic': features[4],
                'connection_count': features[5],
                'peak_frequency': features[6]
            },
            'ai_analysis': {
                'anomaly_score': float(anomaly_score),
                'is_anomaly': bool(is_anomaly),
                'miner_probability': float(miner_probability),
                'is_miner_predicted': bool(is_miner),
                'confidence_level': self._get_confidence_level(miner_probability),
                'pca_representation': pca_features.tolist()
            },
            'recommendations': self._generate_recommendations(anomaly_score, miner_probability)
        }
        
        return result
    
    def _extract_network_features(self, network_data: Dict) -> List[float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡"""
        features = [
            network_data.get('rf_power', -70.0),
            network_data.get('magnetic_field', 50.0),
            network_data.get('temperature', 25.0),
            network_data.get('power_consumption', 200.0),
            network_data.get('network_traffic', 50.0),
            network_data.get('connection_count', 10.0),
            network_data.get('peak_frequency', 50.0)
        ]
        return features
    
    def cluster_analysis(self, data_points: List[Dict]) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        print("ğŸ” ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        if len(data_points) < 5:
            return {
                'error': 'ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª',
                'min_required': 5
            }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        features_matrix = []
        for point in data_points:
            features = self._extract_network_features(point)
            features_matrix.append(features)
        
        features_matrix = np.array(features_matrix)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        features_scaled = self.scaler.transform(features_matrix)
        
        # Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        cluster_labels = self.clustering_model.fit_predict(features_scaled)
        
        # ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
        unique_clusters = set(cluster_labels)
        cluster_analysis = {}
        
        for cluster_id in unique_clusters:
            cluster_points = features_scaled[cluster_labels == cluster_id]
            cluster_size = len(cluster_points)
            
            if cluster_id == -1:  # Ù†ÙˆÛŒØ²
                cluster_type = 'Ù†ÙˆÛŒØ²/Ù¾Ø±Ø§Ú©Ù†Ø¯Ù‡'
                suspicion_level = 'Ù¾Ø§ÛŒÛŒÙ†'
            else:
                # ØªØ­Ù„ÛŒÙ„ Ù…Ø´Ø®ØµØ§Øª Ø®ÙˆØ´Ù‡
                cluster_center = np.mean(cluster_points, axis=0)
                cluster_density = cluster_size / len(features_matrix)
                
                # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø®ÙˆØ´Ù‡
                if cluster_center[3] > 1:  # power consumption Ø¨Ø§Ù„Ø§
                    cluster_type = 'Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ø§ÛŒÙ†Ø± Ø¨Ø§Ù„Ø§'
                    suspicion_level = 'Ø¨Ø§Ù„Ø§'
                elif cluster_center[0] > 0.5:  # RF power Ø¨Ø§Ù„Ø§
                    cluster_type = 'ÙØ¹Ø§Ù„ÛŒØª RF Ù…Ø´Ú©ÙˆÚ©'
                    suspicion_level = 'Ù…ØªÙˆØ³Ø·'
                else:
                    cluster_type = 'ÙØ¹Ø§Ù„ÛŒØª Ø¹Ø§Ø¯ÛŒ'
                    suspicion_level = 'Ù¾Ø§ÛŒÛŒÙ†'
            
            cluster_analysis[f'cluster_{cluster_id}'] = {
                'size': cluster_size,
                'percentage': (cluster_size / len(features_matrix)) * 100,
                'type': cluster_type,
                'suspicion_level': suspicion_level,
                'points_indices': np.where(cluster_labels == cluster_id)[0].tolist()
            }
        
        return {
            'analysis_type': 'cluster_analysis',
            'timestamp': datetime.now().isoformat(),
            'total_points': len(data_points),
            'num_clusters': len(unique_clusters) - (1 if -1 in unique_clusters else 0),
            'noise_points': sum(1 for label in cluster_labels if label == -1),
            'clusters': cluster_analysis,
            'recommendations': self._generate_cluster_recommendations(cluster_analysis)
        }
    
    def temporal_pattern_analysis(self, time_series_data: List[Dict]) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        print("â° ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ...")
        
        if len(time_series_data) < 10:
            return {
                'error': 'ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª',
                'min_required': 10
            }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø®ØªÙ„Ù
        timestamps = [point.get('timestamp', time.time()) for point in time_series_data]
        power_series = [point.get('power_consumption', 0) for point in time_series_data]
        temp_series = [point.get('temperature', 0) for point in time_series_data]
        traffic_series = [point.get('network_traffic', 0) for point in time_series_data]
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        patterns = {
            'power_pattern': self._analyze_time_series(power_series),
            'temperature_pattern': self._analyze_time_series(temp_series),
            'traffic_pattern': self._analyze_time_series(traffic_series)
        }
        
        # ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÛŒ Ù…Ø§ÛŒÙ†Ø± (24/7 ÙØ¹Ø§Ù„ÛŒØª Ø«Ø§Ø¨Øª)
        miner_indicators = 0
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø«Ø¨Ø§Øª Ù…ØµØ±Ù Ø¨Ø±Ù‚
        if patterns['power_pattern']['stability'] > 0.8:
            miner_indicators += 1
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ØµØ±Ù Ø¨Ø§Ù„Ø§ÛŒ Ø¨Ø±Ù‚
        if patterns['power_pattern']['mean'] > 1000:
            miner_indicators += 1
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù…Ø§ÛŒ Ø¨Ø§Ù„Ø§
        if patterns['temperature_pattern']['mean'] > 60:
            miner_indicators += 1
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ±Ø§ÙÛŒÚ© Ø«Ø§Ø¨Øª
        if patterns['traffic_pattern']['stability'] > 0.7:
            miner_indicators += 1
        
        # Ø§Ø­ØªÙ…Ø§Ù„ ÙˆØ¬ÙˆØ¯ Ù…Ø§ÛŒÙ†Ø±
        miner_probability = min(miner_indicators / 4.0, 1.0)
        
        return {
            'analysis_type': 'temporal_pattern',
            'timestamp': datetime.now().isoformat(),
            'data_points': len(time_series_data),
            'time_span_hours': (max(timestamps) - min(timestamps)) / 3600,
            'patterns': patterns,
            'miner_indicators': miner_indicators,
            'miner_probability': miner_probability,
            'assessment': self._assess_temporal_patterns(miner_probability),
            'recommendations': self._generate_temporal_recommendations(patterns, miner_probability)
        }
    
    def _analyze_time_series(self, series: List[float]) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ ÛŒÚ© Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        series_array = np.array(series)
        
        return {
            'mean': float(np.mean(series_array)),
            'std': float(np.std(series_array)),
            'min': float(np.min(series_array)),
            'max': float(np.max(series_array)),
            'trend': self._calculate_trend(series_array),
            'stability': self._calculate_stability(series_array),
            'periodicity': self._detect_periodicity(series_array)
        }
    
    def _calculate_trend(self, series: np.ndarray) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        x = np.arange(len(series))
        slope = np.polyfit(x, series, 1)[0]
        return float(slope)
    
    def _calculate_stability(self, series: np.ndarray) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø«Ø¨Ø§Øª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        if len(series) < 2:
            return 0.0
        
        # Ø¶Ø±ÛŒØ¨ ØªØºÛŒÛŒØ±Ø§Øª (Ú©Ù…â€ŒØªØ± = Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ±)
        cv = np.std(series) / (np.mean(series) + 1e-10)
        stability = max(0, 1 - cv)
        return float(stability)
    
    def _detect_periodicity(self, series: np.ndarray) -> Dict:
        """ØªØ´Ø®ÛŒØµ ØªÙ†Ø§ÙˆØ¨ Ø¯Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        if len(series) < 10:
            return {'has_period': False, 'period': None}
        
        # ØªØ­Ù„ÛŒÙ„ FFT Ø³Ø§Ø¯Ù‡
        fft = np.fft.fft(series)
        freqs = np.fft.fftfreq(len(series))
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ±Ú©Ø§Ù†Ø³ ØºØ§Ù„Ø¨
        dominant_freq_idx = np.argmax(np.abs(fft[1:len(fft)//2])) + 1
        dominant_freq = freqs[dominant_freq_idx]
        
        if abs(dominant_freq) > 0.1:
            period = 1 / abs(dominant_freq)
            return {
                'has_period': True,
                'period': float(period),
                'strength': float(np.abs(fft[dominant_freq_idx]) / np.sum(np.abs(fft)))
            }
        
        return {'has_period': False, 'period': None, 'strength': 0}
    
    def _get_confidence_level(self, probability: float) -> str:
        """ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†"""
        if probability >= 0.8:
            return 'Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§'
        elif probability >= 0.6:
            return 'Ø¨Ø§Ù„Ø§'
        elif probability >= 0.4:
            return 'Ù…ØªÙˆØ³Ø·'
        elif probability >= 0.2:
            return 'Ù¾Ø§ÛŒÛŒÙ†'
        else:
            return 'Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ†'
    
    def _generate_recommendations(self, anomaly_score: float, miner_probability: float) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§"""
        recommendations = []
        
        if miner_probability > 0.7:
            recommendations.append("ğŸš¨ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø§ÛŒÙ†Ø± - Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ ÙÙˆØ±ÛŒ")
            recommendations.append("ğŸ“‹ Ù…Ø³ØªÙ†Ø¯Ø³Ø§Ø²ÛŒ Ø´ÙˆØ§Ù‡Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ù‚Ø¯Ø§Ù… Ù‚Ø§Ù†ÙˆÙ†ÛŒ")
        elif miner_probability > 0.5:
            recommendations.append("âš ï¸ ÙØ¹Ø§Ù„ÛŒØª Ù…Ø´Ú©ÙˆÚ© - Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯")
            recommendations.append("ğŸ” Ø§Ø³Ú©Ù† Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù…Ø­Ø¯ÙˆØ¯Ù‡")
        
        if anomaly_score < -0.5:
            recommendations.append("ğŸ” Ø§Ù„Ú¯ÙˆÛŒ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
            recommendations.append("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ")
        
        if not recommendations:
            recommendations.append("âœ… ÙØ¹Ø§Ù„ÛŒØª Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø·Ø¨ÛŒØ¹ÛŒ")
            recommendations.append("ğŸ”„ Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù…Ø¹Ù…ÙˆÙ„")
        
        return recommendations
    
    def _generate_cluster_recommendations(self, cluster_analysis: Dict) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ"""
        recommendations = []
        
        high_suspicion_clusters = [
            cluster for cluster in cluster_analysis.values()
            if cluster.get('suspicion_level') == 'Ø¨Ø§Ù„Ø§'
        ]
        
        if high_suspicion_clusters:
            recommendations.append(f"ğŸš¨ {len(high_suspicion_clusters)} Ø®ÙˆØ´Ù‡ Ø¨Ø§ Ø³Ø·Ø­ Ø®Ø·Ø± Ø¨Ø§Ù„Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
            recommendations.append("ğŸ” Ø¨Ø±Ø±Ø³ÛŒ ÙÙˆØ±ÛŒ Ù†Ù‚Ø§Ø· Ù…Ø´Ú©ÙˆÚ©")
        
        medium_suspicion_clusters = [
            cluster for cluster in cluster_analysis.values()
            if cluster.get('suspicion_level') == 'Ù…ØªÙˆØ³Ø·'
        ]
        
        if medium_suspicion_clusters:
            recommendations.append(f"âš ï¸ {len(medium_suspicion_clusters)} Ø®ÙˆØ´Ù‡ Ø¨Ø§ Ø³Ø·Ø­ Ø®Ø·Ø± Ù…ØªÙˆØ³Ø·")
        
        return recommendations
    
    def _assess_temporal_patterns(self, miner_probability: float) -> str:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        if miner_probability >= 0.8:
            return "Ø§Ù„Ú¯ÙˆÛŒ ÙØ¹Ø§Ù„ÛŒØª Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ø§ÛŒÙ†Ø± Ø±Ù…Ø²Ø§Ø±Ø²"
        elif miner_probability >= 0.6:
            return "Ø§Ù„Ú¯ÙˆÛŒ ÙØ¹Ø§Ù„ÛŒØª Ø¨Ø³ÛŒØ§Ø± Ù…Ø´Ú©ÙˆÚ© Ø¨Ù‡ Ù…Ø§ÛŒÙ†Ø±"
        elif miner_probability >= 0.4:
            return "Ø§Ù„Ú¯ÙˆÛŒ ÙØ¹Ø§Ù„ÛŒØª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ø¯"
        elif miner_probability >= 0.2:
            return "Ø§Ù„Ú¯ÙˆÛŒ ÙØ¹Ø§Ù„ÛŒØª Ú©Ù…ÛŒ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ"
        else:
            return "Ø§Ù„Ú¯ÙˆÛŒ ÙØ¹Ø§Ù„ÛŒØª Ø·Ø¨ÛŒØ¹ÛŒ"
    
    def _generate_temporal_recommendations(self, patterns: Dict, miner_probability: float) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ù„Ú¯ÙˆÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        recommendations = []
        
        if miner_probability > 0.6:
            recommendations.append("ğŸš¨ Ø§Ù„Ú¯ÙˆÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ø§ÛŒÙ†Ø± - Ø¨Ø±Ø±Ø³ÛŒ ÙÙˆØ±ÛŒ")
            
        if patterns['power_pattern']['mean'] > 2000:
            recommendations.append("âš¡ Ù…ØµØ±Ù Ø¨Ø±Ù‚ Ø¨Ø§Ù„Ø§ - Ø¨Ø±Ø±Ø³ÛŒ ØªØ¬Ù‡ÛŒØ²Ø§Øª")
            
        if patterns['temperature_pattern']['mean'] > 70:
            recommendations.append("ğŸŒ¡ï¸ Ø¯Ù…Ø§ÛŒ Ø¨Ø§Ù„Ø§ - Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ… Ø®Ù†Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡")
            
        if patterns['traffic_pattern']['stability'] > 0.8:
            recommendations.append("ğŸŒ ØªØ±Ø§ÙÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø¸Ù… - Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ø§ÛŒÙ†Ø±")
        
        return recommendations
    
    def save_model(self, filepath: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡"""
        model_data = {
            'anomaly_detector': self.anomaly_detector,
            'pattern_classifier': self.pattern_classifier,
            'scaler': self.scaler,
            'pca_model': self.pca_model,
            'timestamp': datetime.now().isoformat()
        }
        
        joblib.dump(model_data, filepath)
        print(f"ğŸ’¾ Ù…Ø¯Ù„ Ø¯Ø± {filepath} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def load_model(self, filepath: str):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        if os.path.exists(filepath):
            model_data = joblib.load(filepath)
            
            self.anomaly_detector = model_data['anomaly_detector']
            self.pattern_classifier = model_data['pattern_classifier']
            self.scaler = model_data['scaler']
            self.pca_model = model_data['pca_model']
            
            print(f"ğŸ“š Ù…Ø¯Ù„ Ø§Ø² {filepath} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        else:
            print(f"âŒ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ {filepath} ÛŒØ§ÙØª Ù†Ø´Ø¯")

# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
if __name__ == "__main__":
    analyzer = FreeAIAnalyzer()
    
    # Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø¨Ú©Ù‡
    sample_network_data = {
        'rf_power': -45.0,
        'magnetic_field': 120.0,
        'temperature': 75.0,
        'power_consumption': 2500.0,
        'network_traffic': 800.0,
        'connection_count': 150.0,
        'peak_frequency': 27.12
    }
    
    print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...")
    
    # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒ Ø´Ø¨Ú©Ù‡
    network_analysis = analyzer.analyze_network_pattern(sample_network_data)
    print(f"ğŸ•¸ï¸ Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ø§ÛŒÙ†Ø±: {network_analysis['ai_analysis']['miner_probability']:.2f}")
    print(f"ğŸ¯ Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {network_analysis['ai_analysis']['confidence_level']}")
    
    # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ
    sample_points = []
    for i in range(20):
        point = {
            'rf_power': np.random.uniform(-60, -30),
            'magnetic_field': np.random.uniform(40, 150),
            'temperature': np.random.uniform(25, 80),
            'power_consumption': np.random.uniform(200, 3000),
            'network_traffic': np.random.uniform(50, 1000),
            'connection_count': np.random.uniform(5, 200),
            'peak_frequency': np.random.choice([13.56, 27.12, 50.0, 100.0])
        }
        sample_points.append(point)
    
    # ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ
    cluster_analysis = analyzer.cluster_analysis(sample_points)
    print(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§: {cluster_analysis['num_clusters']}")
